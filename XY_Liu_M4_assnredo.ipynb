{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "### The data set is very messy with no exploratory name of attributes, so I add names on top of ever columns in according to the website in which data came from. The name of 26 attributes I added from left to right are: symboling,normalized-losses,make,fuel-type,aspiration,num-of-doors,body-style,drive-wheels,engine-location,wheel-base,length,width,height,curb-weight,engine-type,num-of-cylinders,engine-size,fuel-system,bore,stroke,compression,horsepower, peak-rpm,city-mpg,highway-mpg, price.  \n",
    "\n",
    "#### Symboling corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. In  the dataset, it means the auto which has symboling 3 has highest risk while the auto which has symboling -3 has the lowest risk.\n",
    "\n",
    "#### Normalized-losses is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification, and represents the average loss per car per year. The values range from 65 to 256. \n",
    "\n",
    "#### A car's make is the brand of the vehicle\n",
    "\n",
    "#### A motor fuel is a fuel that is used to provide power to motor vehicles.\n",
    "\n",
    "#### A naturally-aspirated engine, also known as a normally-aspirated engine, is an internal combustion engine in which oxygen intake depends solely on atmospheric pressure and does not have forced induction through a turbocharger or a supercharger. The aspiration with turbo means there is a forced induction device in auto that increases an internal combustion engine's efficiency and power output by forcing extra compressed air into the combustion chamber.\n",
    "\n",
    "#### The fuel system is made up of the fuel tank, pump, filter, and injectors or carburetor, and is responsible for delivering fuel to the engine as needed.\n",
    "\n",
    "#### The bore  is the diameter of each cylinder.\n",
    "\n",
    "#### Horsepower is a unit of power used to measure the forcefulness of a vehicle's engine. The total number of miles the vehicle can go during its lifespan is also determined using horsepower.\n",
    "\n",
    "#### RPM stands for “Revolutions per Minute.” It’s a way of measuring the speed at which the engine revolves or spins.\n",
    "\n",
    "#### City MPG refers to driving with occasional stopping and braking, simulating the conditions you're likely to run into while driving on city streets. Miles per gallon (mpg) is the measurement of the distance in miles that a vehicle can travel using only one gallon of fuel. \n",
    "\n",
    "### In this project, city-mpg is the responsive variable which I want to predict. I try to predict city-mpg using linear regression model, it is my job to choose what exploratory variables are regressors. \n",
    "\n",
    "### The data has plenty of missing values, so I plan to try some methods to deal with missing values.\n",
    "\n",
    "### To simpliy the data without losing too much information, I did feature interaction by multiplying 'width' 'length' 'height' together as 'volume'.\n",
    "\n",
    "### I plotted some graphics to see the linear relationships between city-mpg and other attributes, to see which pairs have more possibility to have linear relationship which could be used as linear model to predict city-mpg.\n",
    "\n",
    "### I also checked the correlations between the regressors I choose to use. Since the multicollinearity exists, I choose to apply PCA on the sampling data, and since the regressors are not continuous, I choose to apply logistic regression model to make the prediction, and check the probability score of my logistic regression model to decide which regressors are the best for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the file and important packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from numpy import linalg as LA\n",
    "from sklearn.experimental import (\n",
    "    enable_iterative_imputer,\n",
    ")\n",
    "from statsmodels.distributions.empirical_distribution  import ECDF\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "seed = 123\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_excel(\"M4_Data.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "#### To begin my data prepocessing, first I want to check what the types of data are in different features, check what are categorical variables, what are numeric variables, what type of numeric values are in numeric variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " #check the types of data \n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After checking data types, I found there are 11 objects, which are categorical attributes, and there are 11 float type and 4 int type, which are numeric attributes. 26 attributes in all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check the data shape\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After checking the data shape, I found there are 205 rows and 26 columns in the whole data set, which means there are 205*26 = 5330 datas in whole, it is pretty a lot.\n",
    "\n",
    "#### Next step, I will check whether there are mistakes and omissions on categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check whether there are mistakes and omissions on categorical variables.\n",
    "classes = ['make','fuel-type','aspiration','num-of-doors','body-style','drive-wheels','engine-location','engine-type','num-of-cylinders','fuel-system']\n",
    "\n",
    "for each in classes:\n",
    "    print(each + ':\\n')\n",
    "    print(list(data[each].drop_duplicates()))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The result shows there are no mistakes and omissions on categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do a descriptive statistics check on data\n",
    "data_desc = data.describe()\n",
    "data_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After descriptive statistic checking on data, I found:\n",
    "\n",
    "#### 1.There are 205 counts in symboling, wheel-base, length, width, height,curb-weight,engine-size,compression,city-mpg, and 203 counts in horsepower,peak-rpm, 201 counts in bore, stroke, price, and 164 counts in normalized-losses, from these information, I realized that the attribute normalized-losses have the highest number of missing values, and bore, stroke, price rank second. When dealing with missing data, the attribute normalized-losses should be put in the most important one.\n",
    "#### 2. Symboling has the lowest mean, price has the highest mean value.\n",
    "#### 3. Price has the largest standard deviation, while bore has the smallest standard deviation. \n",
    "#### 4. Price has the highest min value and symboling has the lowest min value, while price has the highest max value while symboling has the lowest max value.¶\n",
    "\n",
    "#### From descriptive statistic checking, I found there are 6 attributes in numeric attributes which have missing values: horsepower, peak-rpm, bore, stroke, price and normalized-losses. Horsepower and peak-rpm have 2 missing values, bore, stroke and price have 4 missing values, while normalized-losses have 41 missing values.\n",
    "\n",
    "#### To double check the missing value, I apply the code in the following to see how much proportion the missing values owned in comparison with the whole number of data in each specific attribute which has missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_cols = ['normalized-losses','num-of-doors','bore','stroke','horsepower','peak-rpm','price']\n",
    "total_rows = data.shape[0]\n",
    "for each_col in null_cols:\n",
    "    print('{}:{}'.format(each_col,data[pd.isnull(data[each_col])].shape[0]/total_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The result shows in normalized-losses attribute, the missing values owned 20% of the whole data. Bore, stroke and price have around 1.9512% missing values, while num-of-doors, horsepower and peak-rpm have around 0.9756% missing values.\n",
    "\n",
    "#### Data we were dealing with have both numerical and categorical data:\n",
    "#### 1. In numerical data missing values can be replaced with the “average” of the values that the columns have.\n",
    "#### 2. While in categorical data missing value can be handled with “Mode” (frequently occurring) value.\n",
    "\n",
    "### 1. Listwise or case deletion: \n",
    "#### the most common approach to the missing data is to simply omit those cases with the missing data and analyze the remaining data. In num-of-doors, horsepower and peak-rpm, the missing values occupy less than 1% of the whole data, though price has around 1.9512% missing values, but price has little predictale value in predicting city-mpg in reason, therefore, I choose to delete missing values in price, num-of-doors, horsepower and peak-rpm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace('?',np.nan)\n",
    "data.dropna(subset=['price'], axis=0,inplace=True)\n",
    "data.dropna(subset=['horsepower'], axis=0,inplace=True)\n",
    "data.dropna(subset=['peak-rpm'], axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mean substitution: \n",
    "#### In a mean substitution, the mean value of a variable is used in place of the missing data value for that same variable. The theoretical background of the mean substitution is that the mean is a reasonable estimate for a randomly selected observation from a normal distribution. So let's check whether it is suitable to use mean substitution in normalized-losses, bore and stroke by checking their distributions in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram\n",
    "plt.subplot(122)\n",
    "x = data['bore'].dropna()\n",
    "sns.distplot(x,hist=True,kde=True,kde_kws={\"color\":\"k\",\"lw\":3,\"label\":\"KDE\"},hist_kws={\"histtype\":\"step\",\"linewidth\":3,\"alpha\":1,\"color\":\"pink\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram\n",
    "plt.subplot(122)\n",
    "x = data['stroke'].dropna()\n",
    "sns.distplot(x,hist=True,kde=True,kde_kws={\"color\":\"k\",\"lw\":3,\"label\":\"KDE\"},hist_kws={\"histtype\":\"step\",\"linewidth\":3,\"alpha\":1,\"color\":\"pink\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#histogram\n",
    "plt.subplot(122)\n",
    "x = data['normalized-losses'].dropna()\n",
    "sns.distplot(x,hist=True,kde=True,kde_kws={\"color\":\"k\",\"lw\":3,\"label\":\"KDE\"},hist_kws={\"histtype\":\"step\",\"linewidth\":3,\"alpha\":1,\"color\":\"pink\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the histogram I plotted, I could see, bore has two peaks in distribution, which does not satisfy the condition to be normal distribtuion, stroke is pretty fit in normal distribution, and normalized-losses is more closed to skew distribution. Based on the plot I make the conclusion, that I will fill in the missing value of stroke with mean value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "strokemean = data[['stroke']].mean()\n",
    "strokemean\n",
    "data[['stroke']] = data[['stroke']].replace(np.nan,strokemean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filling in missing data using KNN method: the advantage of KNN:\n",
    "\n",
    "#### When dealing with filling in missing values of normalized-losses, I noticed normalized-losses is a skew distribution, so I chose to use KNN imputer to fill in missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data['normalized-losses'].values.reshape(-1, 1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "df = scaler.transform(df)\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "data['normalized-losses'] = imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For bore, I also use KNN imputer to fill in missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data['bore'].values.reshape(-1, 1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "df = scaler.transform(df)\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "data['bore'] = imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning categorical variables into quantitative variables:\n",
    "\n",
    "#### Dummy variable is a numerical variable used to label categories. They are called ‘dummies’ because the numbers themselves don’t have inherent meaning. In the auto data-set, the “fuel-type” feature as a categorical variable has two values, “gas” or “diesel”, which are in String format. For further analysis, it has to convert into some form of numeric format. In the case where the feature “Fuel” has two unique values, gas and diesel, we create two new features ‘gas’ and ‘diesel.’ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummy_fueltype = pd.get_dummies(data['fuel-type'])\n",
    "dummy_fueltype.rename(columns={'fuel-type':'gas','fuel-type':'diesel'},inplace = True)\n",
    "data = pd.concat([data,dummy_fueltype],axis = 1)\n",
    "data.drop('fuel-type',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the auto data-set, the “aspiration” feature as a categorical variable has two values, “std” or “turbo”, which are in String format. For further analysis, it has to convert into some form of numeric format. In the case where the feature “Aspiration” has two unique values, std and turbo, we create two new features ‘std’ and ‘turbo.’ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummy_aspiration = pd.get_dummies(data['aspiration'])\n",
    "dummy_aspiration.rename(columns={'aspiration':'std','aspiration':'turbo'},inplace = True)\n",
    "data = pd.concat([data,dummy_aspiration],axis = 1)\n",
    "data.drop('aspiration',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are some features which have little significance for predictiong the city-mpg, such that: price, number of doors, engine location, so I decide to delete them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.drop('price', axis=1)\n",
    "data = data.drop('num-of-doors', axis=1)\n",
    "data = data.drop('engine-location', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA(Exploratory Data Analysis )\n",
    "### Exploratory Data Analysis,is an approach to analyze data in order to:\n",
    "#### 1. summarize main characteristics of the data\n",
    "#### 2. gain better understanding of the data-set,\n",
    "#### 3. uncover relationships between different variables, and\n",
    "#### 4. extract important variables for the problem we are trying to solve\n",
    "\n",
    "### The question here is to find “What are the characteristics that have the most impact on the city-mpg?”\n",
    "##### We just go through a couple of different useful exploratory data analysis techniques in order to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Do a descriptive statistics check on data\n",
    "data_desc = data.describe()\n",
    "data_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the categorical data by using the function value_counts()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_wheel_count = data['drive-wheels'].value_counts()\n",
    "drive_wheel_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 116 Front-wheel drives, 75 rear-wheel drives, 8 four-wheel drives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_style_count = data['body-style'].value_counts()\n",
    "body_style_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 94 sedan, 67 hatchback, 24 wagons, 8 hardtop and 6 convertibles body style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_type_count = data['engine-type'].value_counts()\n",
    "engine_type_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 143 ohc, 15 ohcf, 13 ohcv, 12 dohc, 12 l and 4 rotor of engine-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberof_cylinders_count = data['num-of-cylinders'].value_counts()\n",
    "numberof_cylinders_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 155 four cylinders, 24 six cylinders, 10 five cylinders, 4 eight cylinders, 4 two cylinders, 1 three cylinders and 1 twelve cylinders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_system_count = data['fuel-system'].value_counts()\n",
    "fuel_system_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 90 mpfi, 64 2bbl, 20 idi, 11 lbbl, 9 spdi, 3 4bbl, 1 spfi and 1 mfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot\n",
    "### Boxplots are a great way to visualize numeric data,the boxplot shows the “median” of the data, which represents where the middle datapoint is. The Upper Quartile shows where the ”75th percentile” is, the Lower Quartile shows where the “25th percentile” is. The data between the Upper and Lower Quartile represents the Interquartile Range(IQR). There are also Lower and Upper Extremes. These are calculated as 1.5 times the interquartile range above the 75th percentile, and as 1.5 times the IQR below the 25th percentile.\n",
    "### Boxplots also display “outliers” as individual dots that occur outside the upper and lower extremes. With boxplots, I can easily spot outliers and also see the distribution and skewness of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='drive-wheels',y='city-mpg',data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From boxplot with x-axis is drive-wheels and y-axis is city-mpg, I make a small conclusion that the auto has front-wheel-drives have highest mean of city-mpg, while the auto has rear-wheel-drives have lowest mean of city-mpg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='engine-type',y='city-mpg',data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From boxplot with x-axis is engine-type and y-axis is city-mpg, I make a small conclusion that the auto has ohc have highest mean of city-mpg, while the auto has rotor and ohcv have lowest mean of city-mpg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='body-style',y='city-mpg',data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From boxplot with x-axis is body-style and y-axis is city-mpg, I make a small conclusion that the auto is hatchback has highest mean of city-mpg, while the auto is convertible has lowest mean of city-mpg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='fuel-system',y='city-mpg',data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From boxplot with x-axis is fuel-system and y-axis is city-mpg, I make a small conclusion that the auto is 2bbl has highest mean of city-mpg, while the auto is 4bbl has lowest mean of city-mpg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='symboling',y='city-mpg',data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From boxplot with x-axis is symboling and y-axis is city-mpg, I make a small conclusion that the auto has symboling 2 has highest mean of city-mpg, while the auto has symboling 3 has lowest mean of city-mpg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction:\n",
    "### In the dataset, I found there are three features: length, width and height which could be extracted as volume just by multiplying length, width and height together, therefore, I will do feature extractiom in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['volume']  = data.length * data.width * data.height\n",
    "data.drop(['width','length','height','curb-weight'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then I want to make a boxplot with mixed features to get better macro visualization:\n",
    "#### Since volume, peak-rpm, normalized-losses, engine-size, wheel-base and horsepower have very large number, for better visualization, I kicked out volume and peak-rpm in the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop(columns=['city-mpg','volume','peak-rpm','normalized-losses','engine-size','horsepower','wheel-base','gas','diesel','turbo','std'])\n",
    "features.plot.box(title='Autocar',vert=False)\n",
    "plt.xticks(rotation = -20)\n",
    "plt.figure()\n",
    "sns.boxplot(data = features, linewidth = 4, orient = 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop(columns=['city-mpg','volume','peak-rpm','turbo','std','gas','diesel','turbo','std','horsepower','wheel-base','compression','stroke','bore','symboling'])\n",
    "features.plot.box(title='Autocar',vert=False)\n",
    "plt.xticks(rotation = -20)\n",
    "plt.figure()\n",
    "sns.boxplot(data = features, linewidth = 4, orient = 'h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the boxplot above, we could see compression, stroke and engine size have more outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot\n",
    "\n",
    "#### The predictor variable: is the variable I am using to predict an outcome. \n",
    "#### The target variable: is the variable I am trying to predict. \n",
    "\n",
    "### In a scatterplot, we typically set the predictor variable on the x-axis, or horizontal axis and we set the target variable on the y-axis or vertical axis.\n",
    "\n",
    "### Let's plot plenty of scatter point to see which features have more tendency to have better linear relationship with city-mpg.\n",
    " \n",
    "#### I will do scatter plot with features in the following sequence: symboling, normalized-losses, wheel-base, volume, curb-weight, engine-size, bore, stroke, compression, horsepower, peak-rpm, highway-mpg. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['symboling']\n",
    "y = data['city-mpg']\n",
    "plt.scatter(x,y)\n",
    "plt.title('Scatter between symboling and city-mpg')\n",
    "plt.xlabel('symboling')\n",
    "plt.ylabel('city-mpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scattor plot between symboling and city-mpg manifest there is hardly a linear relationship between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = data['normalized-losses']\n",
    "y = data['city-mpg']\n",
    "plt.scatter(x,y)\n",
    "plt.title('Scatter between normalized-losses and city-mpg')\n",
    "plt.xlabel('normalized-losses')\n",
    "plt.ylabel('city-mpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scattor plot between normalized-losses and city-mpg manifest there is hardly a linear relationship between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['wheel-base']\n",
    "y = data['city-mpg']\n",
    "plt.scatter(x,y)\n",
    "plt.title('Scatter between wheel-base and city-mpg')\n",
    "plt.xlabel('wheel-base')\n",
    "plt.ylabel('city-mpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scattor plot between wheel-base and city-mpg manifest there may be a negative linear relationship between them, but the linear model is not so apparently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['volume']\n",
    "y = data['city-mpg']\n",
    "plt.scatter(x,y)\n",
    "plt.title('Scatter between volume and city-mpg')\n",
    "plt.xlabel('volume')\n",
    "plt.ylabel('city-mpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scattor plot between volume and city-mpg manifest there may be a negative linear relationship between them, but the points is a little disparse for a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['engine-size']\n",
    "y = data['city-mpg']\n",
    "plt.scatter(x,y)\n",
    "plt.title('Scatter between engine-size and city-mpg')\n",
    "plt.xlabel('engine-size')\n",
    "plt.ylabel('city-mpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scattor plot between engine-size and city-mpg manifest there may be a linear relationship between them, with some outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['bore']\n",
    "y = data['city-mpg']\n",
    "plt.scatter(x,y)\n",
    "plt.title('Scatter between bore and city-mpg')\n",
    "plt.xlabel('bore')\n",
    "plt.ylabel('city-mpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scattor plot between bore and city-mpg manifest there may be a linear relationship between them, but the points are disparse, and outliers exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['stroke']\n",
    "y = data['city-mpg']\n",
    "plt.scatter(x,y)\n",
    "plt.title('Scatter between stroke and city-mpg')\n",
    "plt.xlabel('stroke')\n",
    "plt.ylabel('city-mpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scattor plot between stroke and city-mpg manifest there is hardly a linear relationship between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['compression']\n",
    "y = data['city-mpg']\n",
    "plt.scatter(x,y)\n",
    "plt.title('Scatter between compression and city-mpg')\n",
    "plt.xlabel('compression')\n",
    "plt.ylabel('city-mpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scattor plot between compression and city-mpg manifest there is hardly a linear relationship between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['horsepower']\n",
    "y = data['city-mpg']\n",
    "plt.scatter(x,y)\n",
    "plt.title('Scatter between horsepower and city-mpg')\n",
    "plt.xlabel('horsepower')\n",
    "plt.ylabel('city-mpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scattor plot between horsepower and city-mpg manifest there may be a linear relationship between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['peak-rpm']\n",
    "y = data['city-mpg']\n",
    "plt.scatter(x,y)\n",
    "plt.title('Scatter between peak-rpm and city-mpg')\n",
    "plt.xlabel('peak-rpm')\n",
    "plt.ylabel('city-mpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scattor plot between peak-rpm and city-mpg manifest there is hardly a linear relationship between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also, without checking, city-mpg and highway-mpg must have linear relationship in reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From scatter plot above, we know that the possible linear relatioship pairs are: wheel-base and city-mpg, volume and city-mpg, engine-size and city-mpg, bore and city-mpg, horsepower and city-mpg, highway-mpg and city-mpg. In these pairs, horsepower and city-mpg, highway-mpg and city-mpg are most appropriate for linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gropuby\n",
    "### The “groupby” method groups data by different categories. The data is grouped based on one or several variables and analysis is performed on the individual groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group = data[['wheel-base','volume','city-mpg']]\n",
    "grouped_avg = data_group.groupby(['wheel-base','volume'],as_index=False).mean()\n",
    "pivot_table = grouped_avg.pivot(index = 'wheel-base',columns = 'volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is a great way to plot the target variable over multiple variables and through heatmap get visual clues of the relationship between these variables and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pivot_table)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Variance (ANOVA)\n",
    "### The Analysis of Variance (ANOVA) is a statistical method used to test whether there are significant differences between the means of two or more groups. ANOVA returns two parameters:\n",
    "#### F-test score: ANOVA assumes the means of all groups are the same, calculates how much the actual means deviate from the assumption, and reports it as the F-test score. A larger score means there is a larger difference between the means.\n",
    "#### P-value: P-value tells how statistically significant is our calculated score value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = data[['make','city-mpg']]\n",
    "group_anova = group.groupby(['make'],as_index = False)\n",
    "from scipy import stats\n",
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('audi')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('bmw')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('chevrolet')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('dodge')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('honda')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('isuzu')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('jaguar')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('mazda')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('mercedes-benz')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('mitsubishi')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('nissan')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('peugot')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('plymouth')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('porsche')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('saab')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('subaru')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('toyota')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('volkswagen')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('volvo')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_result = stats.f_oneway(group_anova.get_group('alfa-romero')['city-mpg'],group_anova.get_group('mercury')['city-mpg'])\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I extract the make and city-mpg data. Then I group the data by different makes. \n",
    "#### The anovo_result says city-mpg between alfa-romero and audi are not significantly different, since the F-score is less than 1 and the p-value is larger than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and bmw are not significantly different, since the F-score is less than 1 and the p-value is larger than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and chevrolet are not significantly different, since the F-score is very large but the p-value is less than 0.05.\n",
    "#### The anovo_result says city-mpg between alfa-romero and dodge are significantly different, since the F-score is greater than 1 but the p-value is less than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and honda are significantly different, since the F-score is greater than 1 but the p-value is less than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and isuzu are significantly different, since the F-score is greater than 1 but the p-value is less than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and jaguar significantly different, since the F-score is very large but the p-value is very small.\n",
    "#### The anovo_result says city-mpg between alfa-romero and mazda are significantly different, since the F-score is greater than 1 but the p-value is less than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and mercedes-benz are very significantly different, since the F-score is less than 1 and the p-value is less than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and mitsubishi are significantly different, since the F-score is greater than 1 but the p-value is less than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and nissan are significantly different, since the F-score is greater than 1 but the p-value is less than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and peugot are very significantly different, since the F-score is less than 1 and the p-value is less than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and plymouth are significantly different, since the F-score is greater than 1 but the p-value is less than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and porsche are significantly different, since the F-score is greater than 1 but the p-value is less than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and saab are significantly different, since the F-score is very small and the p-value is larger than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and subaru are significantly different, since the F-score is greater than 1 but the p-value is less than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and toyota are significantly different, since the F-score is greater than 1 but the p-value is larger than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and volkswagen are significantly different, since the F-score is greater than 1 but the p-value is less than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and volvo are very significantly different, since the F-score is less than 1 and the p-value is less than 0.05. \n",
    "#### The anovo_result says city-mpg between alfa-romero and mercury are very significantly different, since the F-score is less than 1 and the p-value is larger than 0.05. \n",
    "\n",
    "\n",
    "## In most cases, different makes have different city-mpg with alfa-romero, which means 'make' takes a very important role in diifference of city-mpg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "### To do multiple linear regression model, it is very important to check whether there are multicollinearities between dfferent independent variables, therefore, it is crucial to check the correlations between feautures which are potential to be the regressors of the linear regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'wheel-base',y = 'volume',data = data)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that the straight line shows that there is a positive linear relationship between wheel-base and volume. With increase in values of wheel-base, volume goes up as well and the slope of the line is positive, so there is a positive correlation between “wheel-base” and \"volume”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'wheel-base',y = 'engine-size',data = data)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that the straight line shows that there is a weak positive linear relationship between wheel-base and engine-size. With increase in values of wheel-base, engine-size goes up as well and the slope of the line is positive, so there is a positive correlation between “wheel-base” and \"engine-size”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'wheel-base',y = 'bore',data = data)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that the straight line shows that there is a very weak positive linear relationship between wheel-base and bore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'wheel-base',y = 'horsepower',data = data)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that the straight line shows that there is a weak positive linear relationship between wheel-base and horsepower. With increase in values of wheel-base, horsepower goes up as well and the slope of the line is positive, so there is a positive correlation between “wheel-base” and \"horsepower”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'wheel-base',y = 'highway-mpg',data = data)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that the straight line shows that there is a weak negative linear relationship between wheel-base and highway-mpg. With increase in values of wheel-base, highway-mpg goes down as well and the slope of the line is negative, so there is a negative correlation between “wheel-base” and \"highway-mpg”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'volume',y = 'engine-size',data = data)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that the straight line shows that there is a strong positive linear relationship between volume and engine-size. With increase in values of volume, engine-size goes up as well and the slope of the line is positive, so there is a positive correlation between “volume” and \"engine-size”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'volume',y = 'bore',data = data)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that the straight line shows that there is a positive linear relationship between volume and bore. With increase in values of volume, bore goes up as well and the slope of the line is positive, so there is a positive correlation between “volume” and \"bore”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'volume',y = 'horsepower',data = data)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that the straight line shows that there is a positive linear relationship between volume and horsepower. With increase in values of volume, horsepower goes up as well and the slope of the line is positive, so there is a positive correlation between “volume” and \"horsepower”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'volume',y = 'highway-mpg',data = data)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that the straight line shows that there is a weak negative linear relationship between volume and highway-mpg. With increase in values of volume, highway-mpg goes down as well and the slope of the line is negative, so there is a weak negative correlation between “volume” and \"highway-mpg”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'engine-size',y = 'bore',data = data)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that the straight line shows that there is a positive linear relationship between engine-size and bore. With increase in values of engine-size, bore goes up as well and the slope of the line is positive, so there is a positive correlation between “engine-size” and \"bore”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'engine-size',y = 'horsepower',data = data)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that the straight line shows that there is a positive linear relationship between engine-size and horsepower. With increase in values of engine-size, horsepower goes up as well and the slope of the line is positive, so there is a positive correlation between “engine-size” and \"horsepower”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'engine-size',y = 'highway-mpg',data = data)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that the straight line shows that there is a weak negative linear relationship between engine-size and highway-mpg. With increase in values of engine-size, highway-mpg goes down as well and the slope of the line is negative, so there is a negative correlation between “engine-size” and \"highway-mpg”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'bore',y = 'horsepower',data = data)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that the straight line shows that there is a weak positive linear relationship between bore and horsepower. With increase in values of bore, horsepower goes up as well and the slope of the line is positive, so there is a positive correlation between “bore” and \"horsepower”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'bore',y = 'highway-mpg',data = data)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that the straight line shows that there is a very weak negative linear relationship between bore and highway-mpg. With increase in values of bore, highway-mpg goes down as well and the slope of the line is negative, so there is a negative correlation between “bore” and \"highway-mpg”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = 'horsepower',y = 'highway-mpg',data = data)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I see that the straight line shows that there is a negative linear relationship between horsepower and highway-mpg. With increase in values of horsepower, highway-mpg goes down as well and the slope of the line is negative, so there is a negative correlation between “horsepower” and \"highway-mpg”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From above, we could pick the weak correlation pairs: wheel-base and engine-size, wheel-base and bore, wheel-base and horsepower, wheel-base and highway-mpg, volume and highway-mpg, engine-size and highway-mpg, bore and horsepower, bore and highway-mpg, horsepower and highway-mpg could be suitable regressors for linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Correlation\n",
    "### The Pearson Correlation measures the linear dependence between two variables X and Y. The resulting coefficient is a value between -1 and 1 inclusive, where:\n",
    "#### 1: total positive linear correlation,\n",
    "#### 0: no linear correlation, the two variables most likely do not affect each other\n",
    "#### -1: total negative linear correlation.\n",
    "\n",
    "#### The P-value is the probability value that the correlation between these two variables is statistically significant. Normally, we choose a significance level of 0.05, which means that we are 95% confident that the correlation between the variables is significant.\n",
    "#### By convention, when the\n",
    "##### p-value is < 0.001 we say there is strong evidence that the correlation is significant,\n",
    "##### the p-value is < 0.05; there is moderate evidence that the correlation is significant,\n",
    "##### the p-value is < 0.1; there is weak evidence that the correlation is significant, and\n",
    "##### the p-value is > 0.1; there is no evidence that the correlation is significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the correlation of the data\n",
    "cor_matrix = data.corr()\n",
    "cor_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let’s calculate the Pearson Correlation Coefficient and P-value of each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coef,p_value = stats.pearsonr(data['wheel-base'],data['engine-size'])\n",
    "print(\"the Pearson Correlation Coefficient is\", pearson_coef, \"with a P-value of P is\",p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the p-value is < 0.001, the correlation between wheel-base and engine-size is statistically significant, although the linear relationship isn’t extremely strong (~0.573)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coef,p_value = stats.pearsonr(data['wheel-base'],data['bore'])\n",
    "print(\"the Pearson Correlation Coefficient is\", pearson_coef, \"with a P-value of P is\",p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the p-value is < 0.001, the correlation between wheel-base and bore is statistically significant, although the linear relationship isn’t extremely strong (~0.478)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coef,p_value = stats.pearsonr(data['wheel-base'],data['horsepower'])\n",
    "print(\"the Pearson Correlation Coefficient is\", pearson_coef, \"with a P-value of P is\",p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the p-value is < 0.001, the correlation between wheel-base and horsepower is statistically significant, although the linear relationship isn’t extremely strong (~0.371)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coef,p_value = stats.pearsonr(data['wheel-base'],data['highway-mpg'])\n",
    "print(\"the Pearson Correlation Coefficient is\", pearson_coef, \"with a P-value of P is\",p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the p-value is < 0.001, the correlation between wheel-base and highway-mpg is statistically significant, although the linear relationship isn’t extremely strong (~-0.542)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coef,p_value = stats.pearsonr(data['volume'],data['highway-mpg'])\n",
    "print(\"the Pearson Correlation Coefficient is\", pearson_coef, \"with a P-value of P is\",p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the p-value is < 0.001, the correlation between volume and highway-mpg is statistically significant, although the linear relationship isn’t extremely strong (~-0.598)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coef,p_value = stats.pearsonr(data['engine-size'],data['highway-mpg'])\n",
    "print(\"the Pearson Correlation Coefficient is\", pearson_coef, \"with a P-value of P is\",p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the p-value is < 0.001, the correlation between engine-size and highway-mpg is statistically significant, although the linear relationship isn’t extremely strong (~-0.680)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As tested, all pairs have strong corrlations, which is not suitable to construct a linear regression model, therefore, I want to apply for PCA. \n",
    "\n",
    "### PCA is effected by scale so I need to scale the features in data before applying PCA. Use StandardScaler to help me standardize the dataset’s features onto unit scale (mean = 0 and variance = 1) which is a requirement for the optimal performance of machine learning algorithms. \n",
    "\n",
    "### Code projects the original data which is 5 dimensional into 2 dimensions. After dimensionality reduction, there usually isn’t a particular meaning assigned to each principal component. The new components are just the two main dimensions of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "features = ['wheel-base', 'engine-size', 'bore', 'volume','horsepower']\n",
    "# Separating out the features\n",
    "x = data.loc[:, features].values\n",
    "# Separating out the target\n",
    "y = data.loc[:,['city-mpg']].values\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating DataFrame along axis = 1. finalDf is the final DataFrame before plotting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, data[['city-mpg']]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize 2D Projection\n",
    "### This section is plotting 2 dimensional data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = ['17', '19', '20']\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['city-mpg'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explained Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By using the attribute explained_variance_ratio_, I can see that the first principal component contains 66.71% of the variance and the second principal component contains 18.37% of the variance. Together, the two components contain 85.08% of the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA to Speed-up Machine Learning Algorithms\n",
    "\n",
    "### Split Data into Training and Test Sets\n",
    "#### In this case, I chose 80% of the data to be training and 20% of the data to be in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['wheel-base','engine-size','bore','volume','horsepower']]\n",
    "Y = data['city-mpg']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2,random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Import and Apply PCA\n",
    "#### Notice the code below has .95 for the number of components parameter. It means that scikit-learn choose the minimum number of principal components such that 95% of the variance is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "pca = PCA(.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit PCA on training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the mapping (transform) to both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Logistic Regression to the Transformed Data\n",
    "\n",
    "### Why do we use logistic regression other than regular linear regression: Because there are input variables with inconsistent numbers of samples.\n",
    "\n",
    "### Properties of Logistic Regression:\n",
    "\n",
    "#### 1.The dependent variable in logistic regression follows Bernoulli Distribution.\n",
    "#### 2. Estimation is done through maximum likelihood.\n",
    "#### 3. No R Square, Model fitness is calculated through Concordance, KS-Statistics.\n",
    "\n",
    "### Linear Regression Vs. Logistic Regression\n",
    "#### Linear regression gives you a continuous output, but logistic regression provides a constant output. \n",
    "#### Linear regression is estimated using Ordinary Least Squares (OLS) while logistic regression is estimated using Maximum Likelihood Estimation (MLE) approach.\n",
    "\n",
    "#### Step 1: Import the model I want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Make an instance of the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Training the model on the data, storing the information learned from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Predict new data \n",
    "#### Uses the information the model learned during the model training process\n",
    "#### The code below predicts for one observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for One Observation\n",
    "logisticRegr.predict(X_test[0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code below predicts for multiple observations at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for One Observation\n",
    "logisticRegr.predict(X_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Model Performance\n",
    "### Accuracy is used here for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The score is low with 0.225, which means the model does not provide a lot of valueable information for predicting, pick another sampling of regressors to see what happens:\n",
    "\n",
    "#### I choose the regressors : wheel-base,engine-size, bore and volume, and check the logistic probability score again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['wheel-base','engine-size','bore','volume']]\n",
    "Y = data['city-mpg']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2,random_state = 10)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test= scaler.transform(X_test)\n",
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "pca = PCA(.95)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(X_train, Y_train)\n",
    "logisticRegr.predict(X_test[0].reshape(1,-1))\n",
    "logisticRegr.predict(X_test[0:10])\n",
    "logisticRegr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The logistic probability score is 0.175"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I choose the regressors : wheel-base,engine-size and bore, and check the logistic probability score again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[['wheel-base','engine-size','bore']]\n",
    "Y = data['city-mpg']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2,random_state = 10)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test= scaler.transform(X_test)\n",
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "pca = PCA(.95)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(X_train, Y_train)\n",
    "logisticRegr.predict(X_test[0].reshape(1,-1))\n",
    "logisticRegr.predict(X_test[0:10])\n",
    "logisticRegr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The logistic probability score is 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I choose the regressors : wheel-base and engine-size, and check the logistic probability score again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['wheel-base','engine-size']]\n",
    "Y = data['city-mpg']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2,random_state = 10)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test= scaler.transform(X_test)\n",
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "pca = PCA(.95)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(X_train, Y_train)\n",
    "logisticRegr.predict(X_test[0].reshape(1,-1))\n",
    "logisticRegr.predict(X_test[0:10])\n",
    "logisticRegr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The logistic probability score is 0.225"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I choose the regressors : wheel-base and horsepower, and check the logistic probability score again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['wheel-base','horsepower']]\n",
    "Y = data['city-mpg']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2,random_state = 10)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test= scaler.transform(X_test)\n",
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "pca = PCA(.95)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(X_train, Y_train)\n",
    "logisticRegr.predict(X_test[0].reshape(1,-1))\n",
    "logisticRegr.predict(X_test[0:10])\n",
    "logisticRegr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The logistic probability score is 0.275"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I choose the regressors : volume and horsepower, and check the logistic probability score again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['volume','horsepower']]\n",
    "Y = data['city-mpg']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2,random_state = 10)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test= scaler.transform(X_test)\n",
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "pca = PCA(.95)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(X_train, Y_train)\n",
    "logisticRegr.predict(X_test[0].reshape(1,-1))\n",
    "logisticRegr.predict(X_test[0:10])\n",
    "logisticRegr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The logistic probability score is 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many times of trying shows that choose 'volume' and 'horsepower' as features to do PCA will give the highest probability score of logistic regression with 0.3. But the score is still high, Let's try to check multicollinearity of features, then apply for linear regression model:\n",
    "\n",
    "### Although correlation matrix and scatter plots can also be used to find multicollinearity, their findings only show the bivariate relationship between the independent variables. VIF is preferred as it can show the correlation of a variable with a group of other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variables</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wheel-base</td>\n",
       "      <td>465.186967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>engine-size</td>\n",
       "      <td>38.852617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bore</td>\n",
       "      <td>227.252473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>volume</td>\n",
       "      <td>226.408347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>horsepower</td>\n",
       "      <td>31.120716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     variables         VIF\n",
       "0   wheel-base  465.186967\n",
       "1  engine-size   38.852617\n",
       "2         bore  227.252473\n",
       "3       volume  226.408347\n",
       "4   horsepower   31.120716"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "def calc_vif(X):\n",
    "\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return(vif)\n",
    "\n",
    "X = data[['wheel-base','engine-size','bore','volume','horsepower']]\n",
    "calc_vif(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the experiment I can see that the ‘wheel-base’ and ‘bore’ and 'volume' have a high VIF value, meaning they can be predicted by other independent variables in the dataset.\n",
    "\n",
    "### Therefore, I can choose engine-size and horsepower to do linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['wheel-base','bore']]\n",
    "Y = data['city-mpg']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2,random_state = 10)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test= scaler.transform(X_test)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see the value of the intercept and slope calculated by the linear regression algorithm for our dataset, execute the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To retrieve the intercept:\n",
    "print(regressor.intercept_)\n",
    "#For retrieving the slope:\n",
    "print(regressor.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The result is -1.95995437 and -2.80233908 respectively.\n",
    "\n",
    "### We have trained our algorithm, it’s time to make some predictions and see how accurately our algorithm predicts the percentage score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now compare the actual output values for X_test with the predicted values, execute the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "dataprediction = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\n",
    "dataprediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also visualize comparison result as a bar graph using the below script :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dataprediction.head(25)\n",
    "df1.plot(kind='bar',figsize=(16,10))\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can observe here that our model has returned a good prediction results.\n",
    "### The final step is to evaluate the performance of the algorithm. We’ll do this by finding the values for MAE, MSE, and RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### From the long experiment, I can make the conclusions as the follow:\n",
    "### 1. In lots of features, engine size and horsepower has less multicollinearity comparing with other features.\n",
    "### 2. All numeric features have correlations, which makes the linear model not the best in this case\n",
    "### 3. When applying the PCA, horsepower also takes an important role, which means horsepower may be the most important indicator in predicting the city-mpg.\n",
    "### The PCA logistic regression has low probability score, so I rejected the PCA logistic regression model.\n",
    "### Though the Mean Absolute Error and Root Mean Squared Error is not as ideal as to be lower than 1, but ###  What I want is a balance between overfit (very low MSE for training data) and underfit (very high MSE for test/validation/unseen data). So the linear regression model was not very accurate but can still make reasonably good predictions.\n",
    "\n",
    "### There are many factors that may have contributed to inaccuracy, for example :\n",
    "#### Need more data: I need to have a huge amount of data to get the best possible prediction.\n",
    "#### Bad assumptions: I made the assumption that this data has a linear relationship, but that might not be the case. \n",
    "#### Poor features: The features I used have had a high enough correlation to the values we were trying to predict.\n",
    "#### Other reasons: As I have tested, different makes have different city-mpg, the categorical features may also affect the linear model but I did not take them into accounts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
